<html>
<head>
<title>06 - Boosting Model.ipynb</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #a9b7c6;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
06 - Boosting Model.ipynb</font>
</center></td></tr></table>
<pre><span class="s0">---
title: &quot;Porto Seguro Insurance claim prediction&quot;
excerpt: &quot;Data science project to create a model to predict the probability of a driver filing an insurance claim 1&lt;br/&gt;&lt;img src='/images/500x300.png'&gt;&quot;
collection: portfolio
---

{
 &quot;cells&quot;: [
  {
   &quot;cell_type&quot;: &quot;markdown&quot;,
   &quot;metadata&quot;: {},
   &quot;source&quot;: [
    &quot;# LightGBM with grid search for hyperparameter optimisation&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;markdown&quot;,
   &quot;metadata&quot;: {},
   &quot;source&quot;: [
    &quot;In this section we are going to create a boosting model for our classification problem.\n&quot;,
    &quot;Boosting is an algorithm which set out to answer the question \&quot;Can a set of weak learners create a single strong learner?\&quot;[1]\n&quot;,
    &quot;It turns out to be very successful in a wide array of applications. [2]\n&quot;,
    &quot;\n&quot;,
    &quot;LightGBM, short for light gradient-boosting machine, is a specific boosting framework developed by microsoft and released open source in 2016.\n&quot;,
    &quot;Although less widely used than XGboost, LightGBM has advantages in efficiency and memory consumption. [3]\n&quot;,
    &quot;\n&quot;,
    &quot;We originally wished to use XGboost, but due to some of the problems we came across when implementing the model, LightGBM was the better choice.&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;code&quot;,
   &quot;execution_count&quot;: 13,
   &quot;metadata&quot;: {
    &quot;collapsed&quot;: true
   },
   &quot;outputs&quot;: [],
   &quot;source&quot;: [
    &quot;import numpy as np\n&quot;,
    &quot;import pandas as pd\n&quot;,
    &quot;from datetime import datetime\n&quot;,
    &quot;from numba import jit\n&quot;,
    &quot;import lightgbm as lgbm\n&quot;,
    &quot;from sklearn.impute import SimpleImputer\n&quot;,
    &quot;from sklearn.model_selection import RandomizedSearchCV\n&quot;,
    &quot;from sklearn.model_selection import StratifiedKFold\n&quot;,
    &quot;from sklearn.preprocessing import StandardScaler\n&quot;,
    &quot;from bayes_opt import BayesianOptimization&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;markdown&quot;,
   &quot;metadata&quot;: {},
   &quot;source&quot;: [
    &quot;Here we have our functions to calculate the gini coefficient, and implement the data handling code to sort out missing values / drop the columns which are mostly missing values and also the calc columns since our EDA discovered these had no correlation to the target. Furthermore, we encode our catagorical features and rescale the data.&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;code&quot;,
   &quot;execution_count&quot;: 14,
   &quot;metadata&quot;: {},
   &quot;outputs&quot;: [],
   &quot;source&quot;: [
    &quot;def timer(start_time=None):\n&quot;,
    &quot;    if not start_time:\n&quot;,
    &quot;        start_time = datetime.now()\n&quot;,
    &quot;        return start_time\n&quot;,
    &quot;    elif start_time:\n&quot;,
    &quot;        thour, temp_sec = divmod((datetime.now() - start_time).total_seconds(), 3600)\n&quot;,
    &quot;        tmin, tsec = divmod(temp_sec, 60)\n&quot;,
    &quot;        print('\\n Time taken: %i hours %i minutes and %s seconds' % (thour, tmin, round(tsec, 2)))\n&quot;,
    &quot;\n&quot;,
    &quot;\n&quot;,
    &quot;@jit\n&quot;,
    &quot;def gini(y_true, y_prob):\n&quot;,
    &quot;    y_true = np.asarray(y_true)\n&quot;,
    &quot;    y_true = y_true[np.argsort(y_prob)]\n&quot;,
    &quot;    ntrue = 0\n&quot;,
    &quot;    gini = 0\n&quot;,
    &quot;    delta = 0\n&quot;,
    &quot;    n = len(y_true)\n&quot;,
    &quot;    for i in range(n - 1, -1, -1):\n&quot;,
    &quot;        y_i = y_true[i]\n&quot;,
    &quot;        ntrue += y_i\n&quot;,
    &quot;        gini += y_i * delta\n&quot;,
    &quot;        delta += 1 - y_i\n&quot;,
    &quot;    gini = 1 - 2 * gini / (ntrue * (n - ntrue))\n&quot;,
    &quot;    return gini\n&quot;,
    &quot;\n&quot;,
    &quot;\n&quot;,
    &quot;def evalerror(preds, dtrain):\n&quot;,
    &quot;    labels = dtrain.get_label()\n&quot;,
    &quot;    return 'gini', gini(labels, preds), True\n&quot;,
    &quot;\n&quot;,
    &quot;\n&quot;,
    &quot;def dropmissingcol(pdData):\n&quot;,
    &quot;    vars_to_drop = ['ps_car_03_cat', 'ps_car_05_cat']\n&quot;,
    &quot;    pdData.drop(vars_to_drop, inplace=True, axis=1)\n&quot;,
    &quot;    return pdData\n&quot;,
    &quot;\n&quot;,
    &quot;\n&quot;,
    &quot;def missingvalues(pdData):\n&quot;,
    &quot;    mean_imp = SimpleImputer(missing_values=-1, strategy='mean')\n&quot;,
    &quot;    mode_imp = SimpleImputer(missing_values=-1, strategy='most_frequent')\n&quot;,
    &quot;    features = ['ps_reg_03', 'ps_car_12', 'ps_car_14', 'ps_car_11']\n&quot;,
    &quot;    for i in features:\n&quot;,
    &quot;        if i == 'ps_car_11':\n&quot;,
    &quot;            pdData[i] = mode_imp.fit_transform(pdData[[i]]).ravel()\n&quot;,
    &quot;        else:\n&quot;,
    &quot;            pdData[i] = mean_imp.fit_transform(pdData[[i]]).ravel()\n&quot;,
    &quot;    return pdData\n&quot;,
    &quot;\n&quot;,
    &quot;\n&quot;,
    &quot;def encodecat(train, test):\n&quot;,
    &quot;    cat_features = [col for col in train.columns if '_cat' in col]\n&quot;,
    &quot;    for column in cat_features:\n&quot;,
    &quot;        temp = pd.get_dummies(pd.Series(train[column]), prefix=column)\n&quot;,
    &quot;        train = pd.concat([train, temp], axis=1)\n&quot;,
    &quot;        train = train.drop([column], axis=1)\n&quot;,
    &quot;\n&quot;,
    &quot;    for column in cat_features:\n&quot;,
    &quot;        temp = pd.get_dummies(pd.Series(test[column]), prefix=column)\n&quot;,
    &quot;        test = pd.concat([test, temp], axis=1)\n&quot;,
    &quot;        test = test.drop([column], axis=1)\n&quot;,
    &quot;    return train, test\n&quot;,
    &quot;\n&quot;,
    &quot;\n&quot;,
    &quot;def RescaleData(train, test):\n&quot;,
    &quot;    scaler = StandardScaler()\n&quot;,
    &quot;    scaler.fit_transform(train)\n&quot;,
    &quot;    scaler.fit_transform(test)\n&quot;,
    &quot;    return train, test\n&quot;,
    &quot;\n&quot;,
    &quot;\n&quot;,
    &quot;def DropCalcCol(train, test):\n&quot;,
    &quot;    col_to_drop = train.columns[train.columns.str.startswith('ps_calc_')]\n&quot;,
    &quot;    train = train.drop(col_to_drop, axis=1)\n&quot;,
    &quot;    test = test.drop(col_to_drop, axis=1)\n&quot;,
    &quot;    return train, test&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;markdown&quot;,
   &quot;metadata&quot;: {},
   &quot;source&quot;: [
    &quot;We begin by loading our data - This is the data where the missing values have already been imputed by a linear regression model.\n&quot;,
    &quot;We can later set our impute boolean to False and compare how effective this model was in comparison to a simple mean/mode imputation.\n&quot;,
    &quot;\n&quot;,
    &quot;Applying the functions above, we have an encoded, rescaled dataset with missing values imputed. The targets have been seperated as new dataframes.\n&quot;,
    &quot;\n&quot;,
    &quot;In general, it is not necessary to scale or encode your data when using boosting algorithms. Boosting algorithms typically work by building a model based on a combination of many weak models, each of which is trained on a subset of the data. This means that boosting algorithms are less sensitive to the scale of the data than some other types of algorithms, such as support vector machines. However, it is unlikely to lower the performance of the algorithm and in some cases could result in increased performance. In this example, it has been done since the functions have already been written for our previous models. Here, lightgbm will be using gradient boosted decision trees which are scale invariant. [4] [5]&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;code&quot;,
   &quot;execution_count&quot;: 15,
   &quot;metadata&quot;: {},
   &quot;outputs&quot;: [],
   &quot;source&quot;: [
    &quot;impute = True #true if we use linear regression to impute missing values - false uses mean/mode imputation\n&quot;,
    &quot;if impute == True: #our imputed training set can be calculated by running the datahandling section of the code\n&quot;,
    &quot;    train = pd.read_csv(\&quot;Dataset/ImputeTrain.csv\&quot;)\n&quot;,
    &quot;    test = pd.read_csv(\&quot;Dataset/ImputeTest.csv\&quot;)\n&quot;,
    &quot;else:\n&quot;,
    &quot;    train = pd.read_csv(\&quot;Dataset/new_train.csv\&quot;)\n&quot;,
    &quot;    test = pd.read_csv(\&quot;Dataset/new_test.csv\&quot;)\n&quot;,
    &quot;    test = dropmissingcol(test)\n&quot;,
    &quot;    train = dropmissingcol(train)\n&quot;,
    &quot;target_test = test['target'].values\n&quot;,
    &quot;test = test.drop(['target'], axis=1)\n&quot;,
    &quot;\n&quot;,
    &quot;#code to clean up any remaining missing values with mean impute\n&quot;,
    &quot;train = missingvalues(train)\n&quot;,
    &quot;test = missingvalues(test)\n&quot;,
    &quot;\n&quot;,
    &quot;#removing these columns from the dataframes and saving them seperately\n&quot;,
    &quot;y_train = train['target'].values\n&quot;,
    &quot;train_id = train['id'].values\n&quot;,
    &quot;X = train.drop(['target', 'id'], axis=1)\n&quot;,
    &quot;test_id = test['id']\n&quot;,
    &quot;X_test = test.drop(['id'], axis=1)\n&quot;,
    &quot;\n&quot;,
    &quot;#encoding, rescaling and dropping calc columns\n&quot;,
    &quot;X, X_test = DropCalcCol(X, X_test)\n&quot;,
    &quot;X, X_test = encodecat(X, X_test)\n&quot;,
    &quot;X = pd.DataFrame(X)\n&quot;,
    &quot;X_test = pd.DataFrame(X_test)\n&quot;,
    &quot;X, X_test = RescaleData(X, X_test)&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;markdown&quot;,
   &quot;metadata&quot;: {},
   &quot;source&quot;: [
    &quot;We are now ready to implement the model.\n&quot;,
    &quot;\n&quot;,
    &quot;XGboost vs LightGBM\n&quot;,
    &quot;\n&quot;,
    &quot;Originally we set out on the project to implement an XGboost algorithm. The reasoning behind this was due to the fact that they often perform extremely well in tasks similar to the one here, especially when looking at the Kaggle leaderboards (albeit prone to overfitting)\n&quot;,
    &quot;When implementing a boosting algorithm, a huge part of the success comes from hyperparameter optimisation. [6]\n&quot;,
    &quot;A method we have previously looked at is Grid Search CV for hyperparameter optimisation - effectively brute searching through a collection of potential hyperparameter combinations and returning the best result. An issue with this method is that the more precision you want, the more combinations and possibilities you will have to try. [7]\n&quot;,
    &quot;\n&quot;,
    &quot;An XGboost algorithm simply fell short on this big dataset as it was going to take a long time to run a grid search.\n&quot;,
    &quot;Possibilities were to reduce the dataset size for the grid search - say run the grid search on 20% of the data,\n&quot;,
    &quot;Or research into LightGBM, an alternative boosting method with solid claims of being a lot more efficient in run time. [8]\n&quot;,
    &quot;\n&quot;,
    &quot;When implementing LightGBM the difference was huge. We could maintain the same full dataset and search through a large amount of hyperparameter combinations to optimise, which resulted in a noticeable score increase in comparison to our xgboost algorithm with a small hyperparameter search grid.\n&quot;,
    &quot;\n&quot;,
    &quot;Therefore, for this project, we found LightGBM to be a better choice of algorithm.&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;markdown&quot;,
   &quot;source&quot;: [
    &quot;# Grid Search&quot;
   ],
   &quot;metadata&quot;: {
    &quot;collapsed&quot;: false
   }
  },
  {
   &quot;cell_type&quot;: &quot;code&quot;,
   &quot;execution_count&quot;: 16,
   &quot;metadata&quot;: {},
   &quot;outputs&quot;: [
    {
     &quot;data&quot;: {
      &quot;text/plain&quot;: &quot;\&quot;\\nstart_time = timer(None)\\n#start search\\nrandom_search.fit(X, y_train)\\ntimer(start_time)\\n\\nprint('\\n All results:')\\nprint(random_search.cv_results_)\\nprint('\\n Best estimator:')\\nprint(random_search.best_estimator_)\\nprint('\\n Best Normalised gini score for %d-fold search with %d parameter combinations:' % (folds, param_comb))\\nprint(random_search.best_score_)\\nprint('\\n Best hyperparameters:')\\nprint(random_search.best_params_)\\nresults = pd.DataFrame(random_search.cv_results_)\\nresults.to_csv('lightgbm-randomgridsearch-results-03.csv')\\n\&quot;&quot;
     },
     &quot;execution_count&quot;: 16,
     &quot;metadata&quot;: {},
     &quot;output_type&quot;: &quot;execute_result&quot;
    }
   ],
   &quot;source&quot;: [
    &quot;OPTIMIZE_ROUNDS = False\n&quot;,
    &quot;LEARNING_RATE = 0.07\n&quot;,
    &quot;EARLY_STOPPING_ROUNDS = 50\n&quot;,
    &quot;\n&quot;,
    &quot;#paramaters to search over\n&quot;,
    &quot;params = {\n&quot;,
    &quot;    'min_child_weight': [5, 10, 12, 15, 30, 50, 100, 150],\n&quot;,
    &quot;    'num_leaves': [4, 5, 8, 10, 15, 20, 30],\n&quot;,
    &quot;    'subsample': [0.2, 0.4, 0.6, 0.8],\n&quot;,
    &quot;    'drop_rate': [0.1, 0.3, 0.5, 0.7, 0.15, 0.2],\n&quot;,
    &quot;    'max_depth': [3, 4, 5, 7, 10, 12, 15, 20]\n&quot;,
    &quot;}\n&quot;,
    &quot;#classifier model\n&quot;,
    &quot;model = lgbm.LGBMClassifier(learning_rate=LEARNING_RATE, n_estimators=600, objective='binary', )\n&quot;,
    &quot;\n&quot;,
    &quot;#folds to use in stratified k-fold\n&quot;,
    &quot;folds = 3\n&quot;,
    &quot;#how many combinations of the above parameters should we try\n&quot;,
    &quot;param_comb = 10\n&quot;,
    &quot;#the algorithm is going to run folds x param_comb times\n&quot;,
    &quot;\n&quot;,
    &quot;SKfold = StratifiedKFold(n_splits=folds, shuffle=True, random_state=1)\n&quot;,
    &quot;#set up search with SKfold split\n&quot;,
    &quot;random_search = RandomizedSearchCV(model, param_distributions=params, n_iter=param_comb, scoring='roc_auc', n_jobs=4,\n&quot;,
    &quot;                                   cv=SKfold.split(X, y_train), verbose=3, random_state=1)\n&quot;,
    &quot;\n&quot;,
    &quot;\n&quot;,
    &quot;#UNCOMMENT THIS SECTION TO START GRID SEARCH - CODE COMPLETION ~4 MIN\n&quot;,
    &quot;'''\n&quot;,
    &quot;start_time = timer(None)\n&quot;,
    &quot;#start search\n&quot;,
    &quot;random_search.fit(X, y_train)\n&quot;,
    &quot;timer(start_time)\n&quot;,
    &quot;\n&quot;,
    &quot;print('\\n All results:')\n&quot;,
    &quot;print(random_search.cv_results_)\n&quot;,
    &quot;print('\\n Best estimator:')\n&quot;,
    &quot;print(random_search.best_estimator_)\n&quot;,
    &quot;print('\\n Best Normalised gini score for %d-fold search with %d parameter combinations:' % (folds, param_comb))\n&quot;,
    &quot;print(random_search.best_score_)\n&quot;,
    &quot;print('\\n Best hyperparameters:')\n&quot;,
    &quot;print(random_search.best_params_)\n&quot;,
    &quot;results = pd.DataFrame(random_search.cv_results_)\n&quot;,
    &quot;results.to_csv('lightgbm-randomgridsearch-results-03.csv')\n&quot;,
    &quot;'''&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;markdown&quot;,
   &quot;metadata&quot;: {},
   &quot;source&quot;: [
    &quot;In grid search, a set of possible values for each hyperparameter is defined, and the combination of these values forms a grid. The grid search algorithm will then train the model with each combination of hyperparameters and evaluate the performance of the model on a validation set. The combination of hyperparameters that produces the best performance on the validation set is chosen as the best set of hyperparameters for the model.\n&quot;,
    &quot;\n&quot;,
    &quot;Grid search can be computationally expensive, as it requires training the model multiple times with different combinations of hyperparameters. However, it is a simple and effective method for finding the best hyperparameters for a given model and dataset.\n&quot;,
    &quot;\n&quot;,
    &quot;Here we have set up a grid search with the hyperparameters of interest to search over. For the remaining hyperparameters, we can make a good guess from LightGBM literature online or they lack importance to fine tune in this particular case.\n&quot;,
    &quot;\n&quot;,
    &quot;In this notebook, we have set to 3 folds and 10 combinations only to make the notebook accessible.\n&quot;,
    &quot;\n&quot;,
    &quot;We run the grid search over a k=5 Stratified k-fold and search through with n_iter as the amount of combinations we wish to look at.\n&quot;,
    &quot;This is still a time intensive exercise, it is run in parallel across 4 chains but the LightGBM model has to train on five folds for each combination, and this is about 20-30 seconds each. For the strongest parameters I will use in the final model, we searched 200 combinations taking around 2 hours. This is overkill&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;markdown&quot;,
   &quot;source&quot;: [
    &quot;# Bayesian Optimisation&quot;
   ],
   &quot;metadata&quot;: {
    &quot;collapsed&quot;: false
   }
  },
  {
   &quot;cell_type&quot;: &quot;markdown&quot;,
   &quot;source&quot;: [
    &quot;Our grid search experienced a lot of flaws regarding the brute search aspect. It just isn't feasible to try every hyperparameter combination possible to really find our best combination. This led us to try a bayesian optimisation method, where in a bayesian way, we start with vague priors with weight on the possible values of the hyperparameters and update that distribution as the search goes on. This allows us to focus around the 'optimum' value much more quickly and waste less time looking at bad combinations. Furthermore, we can find continuous values of our hyperparameters whereas grid search was limited to the values we put into the grid. [9]\n&quot;,
    &quot;\n&quot;,
    &quot;Overall, this is a far superior method for our application here.&quot;
   ],
   &quot;metadata&quot;: {
    &quot;collapsed&quot;: false
   }
  },
  {
   &quot;cell_type&quot;: &quot;code&quot;,
   &quot;execution_count&quot;: 17,
   &quot;metadata&quot;: {},
   &quot;outputs&quot;: [
    {
     &quot;data&quot;: {
      &quot;text/plain&quot;: &quot;'\\n# perform Bayesian optimisation to find the optimal hyperparameters\\noptimizer = BayesianOptimization(evaluate_model, hyperparameters)\\noptimizer.maximize(n_iter=10)\\n\\n# display the optimal values of the hyperparameters\\nprint(\&quot;Optimal hyperparameters:\&quot;)\\nprint(optimizer.max)\\n'&quot;
     },
     &quot;execution_count&quot;: 17,
     &quot;metadata&quot;: {},
     &quot;output_type&quot;: &quot;execute_result&quot;
    }
   ],
   &quot;source&quot;: [
    &quot;def evaluate_model(num_leaves, min_child_weight, feature_fraction, subsample, drop_rate, max_depth):\n&quot;,
    &quot;    params = {\n&quot;,
    &quot;        \&quot;objective\&quot;: \&quot;binary\&quot;,\n&quot;,
    &quot;        \&quot;boosting_type\&quot;: \&quot;gbdt\&quot;,\n&quot;,
    &quot;        \&quot;learning_rate\&quot;: 0.07,\n&quot;,
    &quot;        \&quot;verbosity\&quot;: -1,\n&quot;,
    &quot;        \&quot;num_leaves\&quot;: int(num_leaves),\n&quot;,
    &quot;        \&quot;min_child_weight\&quot;: min_child_weight,\n&quot;,
    &quot;        \&quot;feature_fraction\&quot;: feature_fraction,\n&quot;,
    &quot;        \&quot;subsample\&quot;: subsample,\n&quot;,
    &quot;        'drop_rate': drop_rate,\n&quot;,
    &quot;        'max_depth': int(max_depth)\n&quot;,
    &quot;    }\n&quot;,
    &quot;    num_boost_round = 10000\n&quot;,
    &quot;\n&quot;,
    &quot;    # define the number of folds for cross-validation\n&quot;,
    &quot;    n_folds = 5\n&quot;,
    &quot;\n&quot;,
    &quot;    # create a stratified k-fold iterator\n&quot;,
    &quot;    skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=1)\n&quot;,
    &quot;\n&quot;,
    &quot;    # initialize a list to store the evaluation metric for each fold\n&quot;,
    &quot;    scores = []\n&quot;,
    &quot;\n&quot;,
    &quot;    # iterate over the folds\n&quot;,
    &quot;    for id_train, id_val in skf.split(X, y_train):\n&quot;,
    &quot;        # get the training and validation data for this fold\n&quot;,
    &quot;        X_train_fold = X.iloc[id_train]\n&quot;,
    &quot;        y_train_fold = y_train[id_train]\n&quot;,
    &quot;        X_val_fold = X.iloc[id_val]\n&quot;,
    &quot;        y_val_fold = y_train[id_val]\n&quot;,
    &quot;\n&quot;,
    &quot;        lgb_train = lgbm.Dataset(X_train_fold, y_train_fold)\n&quot;,
    &quot;        lgb_val = lgbm.Dataset(X_val_fold, y_val_fold)\n&quot;,
    &quot;\n&quot;,
    &quot;        # train the model with the specified parameters on the training data\n&quot;,
    &quot;        model = lgbm.train(params, lgb_train, num_boost_round, valid_sets=lgb_val, feval=evalerror, verbose_eval=100, early_stopping_rounds=100)\n&quot;,
    &quot;        scores.append(model.best_score['valid_0']['gini'])\n&quot;,
    &quot;\n&quot;,
    &quot;    # return the mean evaluation metric across all folds\n&quot;,
    &quot;    return np.mean(scores)\n&quot;,
    &quot;\n&quot;,
    &quot;# define the hyperparameters to be optimised\n&quot;,
    &quot;hyperparameters = {\n&quot;,
    &quot;    \&quot;num_leaves\&quot;: (4, 50),\n&quot;,
    &quot;    \&quot;min_child_weight\&quot;: (0.001, 150),\n&quot;,
    &quot;    \&quot;feature_fraction\&quot;: (0.1, 0.9),\n&quot;,
    &quot;    \&quot;subsample\&quot;: (0.1, 1),\n&quot;,
    &quot;    'drop_rate': (0.1, 0.8),\n&quot;,
    &quot;    'max_depth': (3, 20)\n&quot;,
    &quot;}\n&quot;,
    &quot;#UNCOMMENT TO START BAYESIAN OPTIMISATION ~10 MINS\n&quot;,
    &quot;'''\n&quot;,
    &quot;# perform Bayesian optimisation to find the optimal hyperparameters\n&quot;,
    &quot;optimizer = BayesianOptimization(evaluate_model, hyperparameters)\n&quot;,
    &quot;optimizer.maximize(n_iter=10)\n&quot;,
    &quot;\n&quot;,
    &quot;# display the optimal values of the hyperparameters\n&quot;,
    &quot;print(\&quot;Optimal hyperparameters:\&quot;)\n&quot;,
    &quot;print(optimizer.max)\n&quot;,
    &quot;'''&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;code&quot;,
   &quot;execution_count&quot;: 18,
   &quot;outputs&quot;: [
    {
     &quot;data&quot;: {
      &quot;text/plain&quot;: &quot;\&quot;\\nOptimal hyperparameters from bayesian optimisation:\\n{'target': 0.28495605847657257, 'params': {'drop_rate': 0.22059703601445746, 'feature_fraction': 0.5855988837603003, 'max_depth': 17.666370012570326, 'min_child_weight': 139.73583540367778, 'num_leaves': 19.340987296541584, 'subsample': 0.2208063179655893}}\\n\&quot;&quot;
     },
     &quot;execution_count&quot;: 18,
     &quot;metadata&quot;: {},
     &quot;output_type&quot;: &quot;execute_result&quot;
    }
   ],
   &quot;source&quot;: [
    &quot;'''\n&quot;,
    &quot;Best hyperparameters from grid search:\n&quot;,
    &quot;{'subsample': 0.2, 'num_leaves': 15, 'min_child_weight': 150, 'max_depth': 3, 'drop_rate': 0.15}\n&quot;,
    &quot;'''\n&quot;,
    &quot;'''\n&quot;,
    &quot;Optimal hyperparameters from bayesian optimisation:\n&quot;,
    &quot;{'target': 0.28495605847657257, 'params': {'drop_rate': 0.22059703601445746, 'feature_fraction': 0.5855988837603003, 'max_depth': 17.666370012570326, 'min_child_weight': 139.73583540367778, 'num_leaves': 19.340987296541584, 'subsample': 0.2208063179655893}}\n&quot;,
    &quot;'''&quot;
   ],
   &quot;metadata&quot;: {
    &quot;collapsed&quot;: false
   }
  },
  {
   &quot;cell_type&quot;: &quot;markdown&quot;,
   &quot;source&quot;: [
    &quot;# LightGBM model&quot;
   ],
   &quot;metadata&quot;: {
    &quot;collapsed&quot;: false
   }
  },
  {
   &quot;cell_type&quot;: &quot;markdown&quot;,
   &quot;metadata&quot;: {},
   &quot;source&quot;: [
    &quot;Now we have found our best parameters, we are ready to train the model and predict on the test set.\n&quot;,
    &quot;\n&quot;,
    &quot;When working with a dataset that has a class imbalance, stratified k-fold can be especially useful. This is because a dataset with a class imbalance can cause the model to be biased towards the majority class, and the evaluation of the model may be misleading if the folds are not representative of the class distribution in the dataset. By using stratified k-fold cross-validation, the model can be trained and evaluated on balanced folds, which can provide more accurate estimates of the model's performance.\n&quot;,
    &quot;\n&quot;,
    &quot;We also wish to acknowledge the issue of overfitting, so we set early stopping times and iterate this process of generating our predictions by averaging over different folds and furthermore averaging the whole process over different seeds. Taking only our best folds would overfit here. Using multiple models trained with different random seeds will have slightly different parameter values and will make slightly different predictions. Averaging the predictions of these models can help smooth out any overfitting that may have occurred in individual models,&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;code&quot;,
   &quot;execution_count&quot;: 19,
   &quot;metadata&quot;: {},
   &quot;outputs&quot;: [
    {
     &quot;name&quot;: &quot;stdout&quot;,
     &quot;output_type&quot;: &quot;stream&quot;,
     &quot;text&quot;: [
      &quot;\n&quot;,
      &quot; Time taken: 0 hours 0 minutes and 0.0 seconds\n&quot;,
      &quot;Training until validation scores don't improve for 100 rounds\n&quot;,
      &quot;[100]\tvalid_0's binary_logloss: 0.151177\tvalid_0's gini: 0.286179\n&quot;,
      &quot;[200]\tvalid_0's binary_logloss: 0.15118\tvalid_0's gini: 0.286243\n&quot;,
      &quot;Early stopping, best iteration is:\n&quot;,
      &quot;[133]\tvalid_0's binary_logloss: 0.151113\tvalid_0's gini: 0.287715\n&quot;,
      &quot;Training until validation scores don't improve for 100 rounds\n&quot;,
      &quot;[100]\tvalid_0's binary_logloss: 0.150902\tvalid_0's gini: 0.294898\n&quot;,
      &quot;[200]\tvalid_0's binary_logloss: 0.150811\tvalid_0's gini: 0.296848\n&quot;,
      &quot;[300]\tvalid_0's binary_logloss: 0.150866\tvalid_0's gini: 0.296176\n&quot;,
      &quot;Early stopping, best iteration is:\n&quot;,
      &quot;[212]\tvalid_0's binary_logloss: 0.150801\tvalid_0's gini: 0.297337\n&quot;,
      &quot;Training until validation scores don't improve for 100 rounds\n&quot;,
      &quot;[100]\tvalid_0's binary_logloss: 0.15189\tvalid_0's gini: 0.267312\n&quot;,
      &quot;[200]\tvalid_0's binary_logloss: 0.151864\tvalid_0's gini: 0.269072\n&quot;,
      &quot;Early stopping, best iteration is:\n&quot;,
      &quot;[168]\tvalid_0's binary_logloss: 0.151828\tvalid_0's gini: 0.269801\n&quot;,
      &quot;Training until validation scores don't improve for 100 rounds\n&quot;,
      &quot;[100]\tvalid_0's binary_logloss: 0.151287\tvalid_0's gini: 0.285134\n&quot;,
      &quot;[200]\tvalid_0's binary_logloss: 0.151183\tvalid_0's gini: 0.287207\n&quot;,
      &quot;Early stopping, best iteration is:\n&quot;,
      &quot;[166]\tvalid_0's binary_logloss: 0.151178\tvalid_0's gini: 0.28792\n&quot;,
      &quot;Training until validation scores don't improve for 100 rounds\n&quot;,
      &quot;[100]\tvalid_0's binary_logloss: 0.151525\tvalid_0's gini: 0.274378\n&quot;,
      &quot;[200]\tvalid_0's binary_logloss: 0.151466\tvalid_0's gini: 0.276425\n&quot;,
      &quot;Early stopping, best iteration is:\n&quot;,
      &quot;[180]\tvalid_0's binary_logloss: 0.151455\tvalid_0's gini: 0.27675\n&quot;,
      &quot;\n&quot;,
      &quot; Time taken: 0 hours 0 minutes and 57.62 seconds\n&quot;,
      &quot;Training until validation scores don't improve for 100 rounds\n&quot;,
      &quot;[100]\tvalid_0's binary_logloss: 0.1512\tvalid_0's gini: 0.285853\n&quot;,
      &quot;[200]\tvalid_0's binary_logloss: 0.151165\tvalid_0's gini: 0.28648\n&quot;,
      &quot;Early stopping, best iteration is:\n&quot;,
      &quot;[152]\tvalid_0's binary_logloss: 0.151135\tvalid_0's gini: 0.287139\n&quot;,
      &quot;Training until validation scores don't improve for 100 rounds\n&quot;,
      &quot;[100]\tvalid_0's binary_logloss: 0.15095\tvalid_0's gini: 0.29471\n&quot;,
      &quot;[200]\tvalid_0's binary_logloss: 0.150857\tvalid_0's gini: 0.296678\n&quot;,
      &quot;Early stopping, best iteration is:\n&quot;,
      &quot;[155]\tvalid_0's binary_logloss: 0.150852\tvalid_0's gini: 0.296671\n&quot;,
      &quot;Training until validation scores don't improve for 100 rounds\n&quot;,
      &quot;[100]\tvalid_0's binary_logloss: 0.151947\tvalid_0's gini: 0.2655\n&quot;,
      &quot;[200]\tvalid_0's binary_logloss: 0.151902\tvalid_0's gini: 0.268882\n&quot;,
      &quot;Early stopping, best iteration is:\n&quot;,
      &quot;[135]\tvalid_0's binary_logloss: 0.151888\tvalid_0's gini: 0.267791\n&quot;,
      &quot;Training until validation scores don't improve for 100 rounds\n&quot;,
      &quot;[100]\tvalid_0's binary_logloss: 0.151275\tvalid_0's gini: 0.286238\n&quot;,
      &quot;[200]\tvalid_0's binary_logloss: 0.151177\tvalid_0's gini: 0.287693\n&quot;,
      &quot;Early stopping, best iteration is:\n&quot;,
      &quot;[161]\tvalid_0's binary_logloss: 0.151183\tvalid_0's gini: 0.288162\n&quot;,
      &quot;Training until validation scores don't improve for 100 rounds\n&quot;,
      &quot;[100]\tvalid_0's binary_logloss: 0.151525\tvalid_0's gini: 0.275245\n&quot;,
      &quot;[200]\tvalid_0's binary_logloss: 0.151471\tvalid_0's gini: 0.277552\n&quot;,
      &quot;Early stopping, best iteration is:\n&quot;,
      &quot;[147]\tvalid_0's binary_logloss: 0.151438\tvalid_0's gini: 0.277814\n&quot;,
      &quot;\n&quot;,
      &quot; Time taken: 0 hours 1 minutes and 51.69 seconds\n&quot;,
      &quot;Training until validation scores don't improve for 100 rounds\n&quot;,
      &quot;[100]\tvalid_0's binary_logloss: 0.151145\tvalid_0's gini: 0.287636\n&quot;,
      &quot;[200]\tvalid_0's binary_logloss: 0.151226\tvalid_0's gini: 0.285064\n&quot;,
      &quot;Early stopping, best iteration is:\n&quot;,
      &quot;[122]\tvalid_0's binary_logloss: 0.151116\tvalid_0's gini: 0.288137\n&quot;,
      &quot;Training until validation scores don't improve for 100 rounds\n&quot;,
      &quot;[100]\tvalid_0's binary_logloss: 0.151034\tvalid_0's gini: 0.291808\n&quot;,
      &quot;[200]\tvalid_0's binary_logloss: 0.150912\tvalid_0's gini: 0.294779\n&quot;,
      &quot;Early stopping, best iteration is:\n&quot;,
      &quot;[162]\tvalid_0's binary_logloss: 0.150913\tvalid_0's gini: 0.294978\n&quot;,
      &quot;Training until validation scores don't improve for 100 rounds\n&quot;,
      &quot;[100]\tvalid_0's binary_logloss: 0.151839\tvalid_0's gini: 0.270021\n&quot;,
      &quot;[200]\tvalid_0's binary_logloss: 0.151813\tvalid_0's gini: 0.272565\n&quot;,
      &quot;Early stopping, best iteration is:\n&quot;,
      &quot;[148]\tvalid_0's binary_logloss: 0.151771\tvalid_0's gini: 0.272986\n&quot;,
      &quot;Training until validation scores don't improve for 100 rounds\n&quot;,
      &quot;[100]\tvalid_0's binary_logloss: 0.151356\tvalid_0's gini: 0.283382\n&quot;,
      &quot;[200]\tvalid_0's binary_logloss: 0.151151\tvalid_0's gini: 0.287289\n&quot;,
      &quot;Early stopping, best iteration is:\n&quot;,
      &quot;[173]\tvalid_0's binary_logloss: 0.151163\tvalid_0's gini: 0.287526\n&quot;,
      &quot;Training until validation scores don't improve for 100 rounds\n&quot;,
      &quot;[100]\tvalid_0's binary_logloss: 0.151501\tvalid_0's gini: 0.27657\n&quot;,
      &quot;[200]\tvalid_0's binary_logloss: 0.151426\tvalid_0's gini: 0.279622\n&quot;,
      &quot;[300]\tvalid_0's binary_logloss: 0.151463\tvalid_0's gini: 0.278492\n&quot;,
      &quot;Early stopping, best iteration is:\n&quot;,
      &quot;[207]\tvalid_0's binary_logloss: 0.151414\tvalid_0's gini: 0.279792\n&quot;,
      &quot;Score on the test data\n&quot;,
      &quot;Gini\n&quot;,
      &quot;0.2817052971888222\n&quot;
     ]
    }
   ],
   &quot;source&quot;: [
    &quot;#use our best parameters\n&quot;,
    &quot;'''\n&quot;,
    &quot;Optimal hyperparameters from bayesian optimisation:\n&quot;,
    &quot;{'target': 0.28495605847657257, 'params': {'drop_rate': 0.22059703601445746, 'feature_fraction': 0.5855988837603003, 'max_depth': 17.666370012570326, 'min_child_weight': 139.73583540367778, 'num_leaves': 19.340987296541584, 'subsample': 0.2208063179655893}}\n&quot;,
    &quot;'''\n&quot;,
    &quot;\n&quot;,
    &quot;min_data_in_leaf = 2000\n&quot;,
    &quot;num_boost_round = 10000\n&quot;,
    &quot;params = {\&quot;objective\&quot;: \&quot;binary\&quot;,\n&quot;,
    &quot;          \&quot;boosting_type\&quot;: \&quot;gbdt\&quot;,\n&quot;,
    &quot;          \&quot;learning_rate\&quot;: 0.07,\n&quot;,
    &quot;          \&quot;max_bin\&quot;: 256,\n&quot;,
    &quot;          \&quot;verbosity\&quot;: -1,\n&quot;,
    &quot;          \&quot;feature_fraction\&quot;: 0.5855988837603003,\n&quot;,
    &quot;          'subsample': 0.220806,\n&quot;,
    &quot;          'num_leaves': 19,\n&quot;,
    &quot;          'min_child_weight': 140,\n&quot;,
    &quot;          'max_depth': 18,\n&quot;,
    &quot;          'drop_rate': 0.22059703601445746\n&quot;,
    &quot;          }\n&quot;,
    &quot;\n&quot;,
    &quot;folds = 5\n&quot;,
    &quot;\n&quot;,
    &quot;SKfold = StratifiedKFold(n_splits=folds, shuffle=True, random_state=1)\n&quot;,
    &quot;\n&quot;,
    &quot;#empty, will save our scores here in the future\n&quot;,
    &quot;best_trees = []\n&quot;,
    &quot;fold_scores = []\n&quot;,
    &quot;\n&quot;,
    &quot;#cv_train = np.zeros(len(y_train))\n&quot;,
    &quot;cv_pred = np.zeros(len(X_test))\n&quot;,
    &quot;\n&quot;,
    &quot;start_time = timer(None)\n&quot;,
    &quot;#iterations each have a different seed, we average over these to prevent overfit\n&quot;,
    &quot;iterations = 3\n&quot;,
    &quot;for seed in range(iterations):\n&quot;,
    &quot;    timer(start_time)\n&quot;,
    &quot;    params['seed'] = seed\n&quot;,
    &quot;    #start SK fold\n&quot;,
    &quot;    for id_train, id_test in SKfold.split(X, y_train):\n&quot;,
    &quot;        #x train, x validation\n&quot;,
    &quot;        xtr, xvl = X.loc[id_train], X.loc[id_test]\n&quot;,
    &quot;        #y train, y validation\n&quot;,
    &quot;        ytr, yvl = y_train[id_train], y_train[id_test]\n&quot;,
    &quot;        #efficient datastructures for lgbm\n&quot;,
    &quot;        dtrain = lgbm.Dataset(data=xtr, label=ytr)\n&quot;,
    &quot;        dval = lgbm.Dataset(data=xvl, label=yvl, reference=dtrain)\n&quot;,
    &quot;        #model training\n&quot;,
    &quot;        bst = lgbm.train(params, dtrain, num_boost_round, valid_sets=dval, feval=evalerror, verbose_eval=100,\n&quot;,
    &quot;                         early_stopping_rounds=100)\n&quot;,
    &quot;        #add best tree and fold scores to data structure\n&quot;,
    &quot;        best_trees.append(bst.best_iteration)\n&quot;,
    &quot;        fold_scores.append(bst.best_score)\n&quot;,
    &quot;        #predict for our test set with best tree from fold. Sums the probabilities\n&quot;,
    &quot;        cv_pred += bst.predict(X_test, num_iteration=bst.best_iteration)\n&quot;,
    &quot;    #average the predictions for our 5 folds\n&quot;,
    &quot;\n&quot;,
    &quot;pd.DataFrame({'id': test_id, 'target': cv_pred / (iterations * folds)}).to_csv('Results/MeanImpute/lgbm.csv', index=False)\n&quot;,
    &quot;\n&quot;,
    &quot;test_score = gini(target_test, cv_pred / (iterations * folds))\n&quot;,
    &quot;print(\&quot;Score on the test data\&quot;)\n&quot;,
    &quot;print(\&quot;Gini\&quot;)\n&quot;,
    &quot;print(test_score)&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;markdown&quot;,
   &quot;metadata&quot;: {},
   &quot;source&quot;: [
    &quot;Our final result on the test set is&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;code&quot;,
   &quot;execution_count&quot;: 20,
   &quot;metadata&quot;: {},
   &quot;outputs&quot;: [
    {
     &quot;data&quot;: {
      &quot;text/plain&quot;: &quot;'0.2817052971888222'&quot;
     },
     &quot;execution_count&quot;: 20,
     &quot;metadata&quot;: {},
     &quot;output_type&quot;: &quot;execute_result&quot;
    }
   ],
   &quot;source&quot;: [
    &quot;'GRID SEARCH PARAMETERS'\n&quot;,
    &quot;\&quot;Score on the test data\&quot;\n&quot;,
    &quot;\&quot;Gini\&quot;\n&quot;,
    &quot;\&quot;0.2788912296158311\&quot;\n&quot;,
    &quot;\n&quot;,
    &quot;'BAYESIAN OPTIMISATION PARAMETERS'\n&quot;,
    &quot;\&quot;Score on the test data\&quot;\n&quot;,
    &quot;\&quot;Gini\&quot;\n&quot;,
    &quot;\&quot;0.2817052971888222\&quot;&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;markdown&quot;,
   &quot;metadata&quot;: {},
   &quot;source&quot;: [
    &quot;Separately, we run the same code with the mean imputed data and gridsearch parameters and our score is 0.2704817256788792&quot;
   ]
  },
  {
   &quot;cell_type&quot;: &quot;markdown&quot;,
   &quot;source&quot;: [
    &quot;# References&quot;
   ],
   &quot;metadata&quot;: {
    &quot;collapsed&quot;: false
   }
  },
  {
   &quot;cell_type&quot;: &quot;markdown&quot;,
   &quot;source&quot;: [
    &quot;[1] https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=4809227\n&quot;,
    &quot;Vaghela, Ganatra, Thakkar - Boost a Weak Learner to a Strong Learner Using Ensemble System Approach - 2009 - DOI:10.1109/IADCC.2009.4809227\n&quot;,
    &quot;[2] https://www.toptal.com/machine-learning/ensemble-methods-kaggle-machine-learn\n&quot;,
    &quot;[3] https://lightgbm.readthedocs.io/en/latest/Features.html#optimization-in-speed-and-memory-usage\n&quot;,
    &quot;[4] https://sebastianraschka.com/faq/docs/when-to-standardize.html\n&quot;,
    &quot;[5] https://en.wikipedia.org/wiki/Decision_tree_learning#Advantages\n&quot;,
    &quot;[6] https://towardsdatascience.com/understanding-hyperparameters-and-its-optimisation-techniques-f0debba07568\n&quot;,
    &quot;[7] https://web.archive.org/web/20160701182750/http://blog.dato.com/how-to-evaluate-machine-learning-models-part-4-hyperparameter-tuning\n&quot;,
    &quot;[8] https://www.geeksforgeeks.org/lightgbm-vs-xgboost-which-algorithm-is-better/\n&quot;,
    &quot;[9] http://neupy.com/2016/12/17/hyperparameter_optimization_for_neural_networks.html\n&quot;,
    &quot;\n&quot;,
    &quot;\n&quot;,
    &quot;\n&quot;,
    &quot;\n&quot;,
    &quot;\n&quot;,
    &quot;\n&quot;,
    &quot;\n&quot;
   ],
   &quot;metadata&quot;: {
    &quot;collapsed&quot;: false
   }
  }
 ],
 &quot;metadata&quot;: {
  &quot;kernelspec&quot;: {
   &quot;display_name&quot;: &quot;Python 3&quot;,
   &quot;language&quot;: &quot;python&quot;,
   &quot;name&quot;: &quot;python3&quot;
  },
  &quot;language_info&quot;: {
   &quot;codemirror_mode&quot;: {
    &quot;name&quot;: &quot;ipython&quot;,
    &quot;version&quot;: 3
   },
   &quot;file_extension&quot;: &quot;.py&quot;,
   &quot;mimetype&quot;: &quot;text/x-python&quot;,
   &quot;name&quot;: &quot;python&quot;,
   &quot;nbconvert_exporter&quot;: &quot;python&quot;,
   &quot;pygments_lexer&quot;: &quot;ipython3&quot;,
   &quot;version&quot;: &quot;3.8.5&quot;
  }
 },
 &quot;nbformat&quot;: 4,
 &quot;nbformat_minor&quot;: 1
}
</span></pre>
</body>
</html>
